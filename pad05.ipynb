{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Zadanie 1 (1 pkt)\n",
    "Wejdź na stronę https://www.pap.pl i sprawdź czy pozwala ona na web scraping i sprawdź czy pozwala ona na scraping sprawdzając plik `robots.txt`.\n",
    "Wklej tutaj jego treść.\n",
    "```\n",
    "User-agent: *\n",
    "# Directories\n",
    "Disallow: /wyszukiwanie/\n",
    "Disallow: /en/search/\n",
    "Disallow: /core/\n",
    "Disallow: /profiles/\n",
    "Disallow: /node/\n",
    "Disallow: /en/node/\n",
    "Disallow: /ru/node/\n",
    "Disallow: /ua/node/\n",
    "# Files\n",
    "Disallow: /README.txt\n",
    "Disallow: /web.config\n",
    "# Paths (clean URLs)\n",
    "Disallow: /admin/\n",
    "Disallow: /comment/\n",
    "Disallow: /filter/\n",
    "Disallow: /search/\n",
    "Disallow: /user/\n",
    "Disallow: /media/oembed\n",
    "Disallow: /*/media/oembed\n",
    "Disallow: /index.php/\n",
    "```\n",
    "Strona nie zabrania web scrapingu całkowicie, jednak ogranicza dostęp do pewnych sekcji.\n",
    "Gdyby web scraping był całkowicie zakazany, plik wyglądał by tak:\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Zadanie 2 (8 pkt)\n",
    "Stwórz obiekt driver, który połączy się ze stroną PAP, a następnie użyj go żeby wykonać następujące akcje:\n",
    "1. Zaakceptuj pliki cookies\n",
    "2. Zmaksymalizuj okno przeglądarki\n",
    "3. Zmień język strony na angielski\n",
    "4. Wejdź w sekcję \"Business\"\n",
    "5. Z sekcji \"Business\" ściągnij wszystkie tytuły do listy _titles_\n",
    "6. Ściągnij wszystkie zdjęcia z tej sekcji\n",
    "7. Zejdź na dół strony\n",
    "8. Przejdź na ostatnią stronę i zwróć jej numer (atrybut `text`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.FirefoxOptions()\n",
    "options.add_argument(\"--incognito\")  # okno incognito żeby przy kolejnych uruchomieniach nie mieć zapamiętanych cookies\n",
    "\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.get(\"https://www.pap.pl\")\n",
    "\n",
    "# 1\n",
    "driver.find_element(By.CLASS_NAME, \"ok.closeButton\").click()\n",
    "\n",
    "# 2\n",
    "driver.maximize_window()\n",
    "\n",
    "# 3\n",
    "driver.find_element(By.ID, \"lang-switch\").click()\n",
    "driver.find_element(By.CLASS_NAME, \"choice-en\").click()\n",
    "\n",
    "# 4\n",
    "driver.find_element(By.XPATH, \"//a[@href='/en/business']\").click()\n",
    "\n",
    "# 5\n",
    "titles: List[str] = []\n",
    "articles = driver.find_elements(By.CLASS_NAME, \"title\")\n",
    "for article in articles:\n",
    "    titles.append(article.text)\n",
    "print(titles)\n",
    "\n",
    "# 6\n",
    "imgs_out = \"data/05/zad6_out/\"\n",
    "if not os.path.exists(imgs_out):\n",
    "    os.makedirs(imgs_out)\n",
    "\n",
    "images = driver.find_elements(By.XPATH,\n",
    "                              \"//div[contains(@class, 'newsList')]//div[contains(@class, 'imageWrapper')]//img\")\n",
    "image_urls: List[str] = []\n",
    "for idx, img in enumerate(images):\n",
    "    img_url = img.get_attribute(\"src\")\n",
    "    if img_url:\n",
    "        try:\n",
    "            print(f\"Downloading {img_url}\")\n",
    "            img_content = requests.get(img_url).content\n",
    "            img_filename = os.path.join(imgs_out, f\"img_{idx}.jpg\")\n",
    "            with open(img_filename, \"wb\") as f:\n",
    "                f.write(img_content)\n",
    "            print(f\"Saved to {img_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Got error {e}\")\n",
    "\n",
    "# 7\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "# 8\n",
    "driver.find_element(By.XPATH, \"//li[contains(@class, 'pager__item--last')]/a\").click()\n",
    "# Trzeba wybrać element od nowa po przełączeniu strony, bo przed przełączeniem tekst to \"Ostatnia >>\", a nie numer strony\n",
    "current_page = driver.find_element(By.XPATH,\n",
    "                                   \"//li[contains(@class, 'pager__item') and contains(@class, 'is-active') and contains(@class, 'active')]\")\n",
    "print(current_page.text)\n",
    "\n",
    "# zamykamy przeglądarkę na koniec, małe opóźnienie aby zobaczyć ostatni efekt\n",
    "time.sleep(2)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Zadanie 3 (1 pkt)\n",
    "Wybierz stronę, z której będziesz pobierać dane do projektu końcowego.\n",
    "\n",
    "Projekt w parze z Maciejem Niszczotą (s32679)\n",
    "\n",
    "Propozycja tematu nr 1: GTFS feed (mobilitydatabase.org). Eksperyment czy można prognozować opóźnienia jedynie na podstawie statycznych rozkładów jazdy?\n",
    "Weryfikacja hipotezy z użyciem feedów GTFS RT (real time). W tym przypadku dane są pobierane z api, a nie stricte scrapowane.\n",
    "Dokumentacja standardu GTFS: https://gtfs.org/\n",
    "\n",
    "Propozycja tematu nr 2: Prognoza czasów sprzedaży pojazdów (otomoto.pl)\n",
    "\n",
    "Propozycja tematu nr 3: Prognoza czasów sprzedaży bądź cen mieszkań (otodom.pl). W przypadku prognoz będzie można zweryfikować wyniki z dostępnymi statystykami odnośnie\n",
    "rynku mieszkaniowego.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
